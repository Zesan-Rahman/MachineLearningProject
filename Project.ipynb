{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Source**: German Credit Data (`credit-g` from OpenML, dataset ID 31)  \n",
    "**File**: `dataset_31_credit-g.arff`  \n",
    "**Size**: 1,000 instances with 21 attributes (20 features + 1 target)\n",
    "\n",
    "The dataset contains information about credit applicants with a mix of categorical and numerical features:\n",
    "- **Numerical features (7)**: duration, credit_amount, installment_commitment, residence_since, age, existing_credits, num_dependents\n",
    "- **Categorical features (13)**: checking_status, credit_history, purpose, savings_status, employment, personal_status, other_parties, property_magnitude, other_payment_plans, housing, job, own_telephone, foreign_worker\n",
    "- **Target variable**: `class` (binary: \"good\" or \"bad\" credit risk)\n",
    "\n",
    "## Machine Learning Task\n",
    "\n",
    "**Task Type**: Binary Classification  \n",
    "**Objective**: Predict whether an individual is a good or bad credit risk based on their financial and personal attributes.\n",
    "\n",
    "**Why This Task Matters**:  \n",
    "Credit risk assessment is a critical real-world application in financial institutions. Accurate classification helps:\n",
    "- Reduce financial losses from defaults\n",
    "- Make fair and consistent lending decisions\n",
    "- Identify both obvious and subtle risk factors\n",
    "- Balance approval rates with risk management\n",
    "\n",
    "Misclassifying a bad credit risk as good (false positive) is particularly costly as it can lead to loan defaults, while misclassifying a good applicant as bad (false negative) results in lost business opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ARFF file\n",
    "data, meta = arff.loadarff('dataset_31_credit-g.arff')\n",
    "df = pd.DataFrame(data)\n",
    "#Getting rid of byte strings\n",
    "for col in df.select_dtypes([object]):\n",
    "    df[col] = df[col].str.decode('utf-8')\n",
    "#print(df) #test to see if df works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Distribution Visualization\n",
    "\n",
    "Visualizing the distribution of key features to understand the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Histograms of Key Numerical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVIDER = \"===================================================\"\n",
    "#Interesting histograms\n",
    "print(DIVIDER)\n",
    "print(\"Histograms\")\n",
    "print(DIVIDER)\n",
    "for col in df.columns:\n",
    "    if col in [\"duration\", \"credit_amount\",\"age\"]:\n",
    "        plt.hist(df[col], bins=30, color='skyblue', edgecolor='black')\n",
    "        plt.title(f\"Distribution: {col}\")\n",
    "        plt.xlabel('Values')\n",
    "        plt.ylabel('Frequencies')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Correlation Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "print(DIVIDER)\n",
    "print(\"Correlation Matrix\")\n",
    "print(DIVIDER)\n",
    "\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "sns.heatmap(corr, annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Relationships with Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationships with target variable\n",
    "print(DIVIDER)\n",
    "print(\"Relationship with target variable\")\n",
    "print(DIVIDER)\n",
    "target_col = 'class'\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = df.select_dtypes(include='number').columns\n",
    "categorical_features = df.select_dtypes(exclude='number').columns\n",
    "\n",
    "# Numeric features -> boxplot vs class\n",
    "for feature in numeric_features:\n",
    "    sns.boxplot(x=target_col, y=feature, data=df)\n",
    "    plt.title(f'{feature} vs {target_col}')\n",
    "    plt.show()\n",
    "\n",
    "# Categorical features -> countplot vs class\n",
    "for feature in categorical_features:\n",
    "    sns.countplot(x=feature, hue=target_col, data=df)\n",
    "    plt.title(f'{feature} vs {target_col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This section covers the preprocessing steps needed for supervised modeling in Part C.\n",
    "\n",
    "**Tasks:**\n",
    "1. Handle missing values\n",
    "2. Apply transformations (log1p) to skewed features\n",
    "3. Encode categorical variables\n",
    "4. Scale numerical features\n",
    "5. Split data into Training (70%), Validation (20%), and Test (10%) sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "target_col = 'class'\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Encode target (good/bad) to 1/0\n",
    "y = LabelEncoder().fit_transform(df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Check Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness of numerical columns\n",
    "numerical_cols_check = df.select_dtypes(include=['number']).columns\n",
    "skewness = df[numerical_cols_check].skew()\n",
    "print(\"Skewness of numerical features:\")\n",
    "print(skewness.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the features\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "print(f\"\\nProcessed feature matrix shape: {X_processed_df.shape}\")\n",
    "print(f\"  - Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  - One-hot encoded features: {len(cat_feature_names)}\")\n",
    "X_processed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Structure Check (PCA + K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# PCA to 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_processed_df)\n",
    "pca_explained = pca.explained_variance_ratio_\n",
    "print(f\"PC1 variance: {pca_explained[0]:.2%}\")\n",
    "print(f\"PC2 variance: {pca_explained[1]:.2%}\")\n",
    "print(f\"Cumulative (PC1+PC2): {pca_explained.sum():.2%}\")\n",
    "\n",
    "# K-Means on PCA projection\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=20)\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "sil_score = silhouette_score(X_pca, cluster_labels)\n",
    "cluster_mix = pd.crosstab(cluster_labels, y, normalize='index')\n",
    "print(f\"Silhouette score (k=2 on PCA space): {sil_score:.3f}\")\n",
    "print(\"\\nCluster composition by true class (row-normalized):\")\n",
    "print(cluster_mix)\n",
    "\n",
    "# Visualize PCA projection with labels and clusters\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['class'] = y\n",
    "pca_df['cluster'] = cluster_labels\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='class', palette='Set1', ax=axes[0], s=25)\n",
    "axes[0].set_title('PCA projection colored by true class')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].legend(title='class', labels=['Bad (0)', 'Good (1)'])\n",
    "\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='cluster', palette='Set2', ax=axes[1], s=25)\n",
    "axes[1].set_title('K-Means clusters (k=2) in PCA space')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].legend(title='cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 80% Train, 20% Temp (Validation + Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_processed_df, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: split the 20% Temp into 1/2 Validation (10% total) and 1/2 Test (10% total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape} ({len(X_train)/len(df):.1%})\")\n",
    "print(f\"Validation set shape: {X_val.shape} ({len(X_val)/len(df):.1%})\")\n",
    "print(f\"Test set shape: {X_test.shape} ({len(X_test)/len(df):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "log_reg_results = []\n",
    "\n",
    "def run_log_reg(X_train_z, y_train, X_val_z, y_val, X_test_z, y_test, name):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    threshold = 0.7\n",
    "\n",
    "    for lambda_val in lambda_values:\n",
    "        model = LogisticRegression(max_iter=2000, C=lambda_val)\n",
    "        model.fit(X_train_z, y_train)\n",
    "\n",
    "        # Apply threshold on validation set \n",
    "        probs_val = model.predict_proba(X_val_z)[:, 1]\n",
    "        y_val_pred_thresh = (probs_val >= threshold).astype(int)\n",
    "\n",
    "        # Accuracy using thresholded predictions\n",
    "        val_acc = accuracy_score(y_val, y_val_pred_thresh)\n",
    "        val_f1 = f1_score(y_val, y_val_pred_thresh)\n",
    "        val_roc = roc_auc_score(y_val, probs_val)\n",
    "\n",
    "        # Apply threshold on test set\n",
    "        probs_test = model.predict_proba(X_test_z)[:, 1]\n",
    "        y_test_pred_thresh = (probs_test >= threshold).astype(int)\n",
    "\n",
    "        test_acc = accuracy_score(y_test, y_test_pred_thresh)\n",
    "        test_f1 = f1_score(y_test, y_test_pred_thresh)\n",
    "        test_roc = roc_auc_score(y_test, probs_test)\n",
    "\n",
    "        print(f\"lambda={lambda_val:<6} | Val Acc = {val_acc:.4f} | Test Acc = {test_acc:.4f}\")\n",
    "        \n",
    "        log_reg_results.append({\n",
    "            'feature_version': name,\n",
    "            'lambda': lambda_val,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1,\n",
    "            'val_roc_auc': val_roc,\n",
    "            'test_acc': test_acc,\n",
    "            'test_f1': test_f1,\n",
    "            'test_roc_auc': test_roc\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z0: Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_log_reg(X_train, y_train, X_val, y_val, X_test, y_test, \"Z0: Original Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z1: Squared Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_train = X_train.copy()\n",
    "Z1_val = X_val.copy()\n",
    "Z1_test = X_test.copy()\n",
    "\n",
    "# Choose features to square\n",
    "square_features = [\"credit_amount\", \"age\"]\n",
    "\n",
    "for col in square_features:\n",
    "    Z1_train[col + \"_sq\"] = Z1_train[col] ** 2\n",
    "    Z1_val[col + \"_sq\"] = Z1_val[col] ** 2\n",
    "    Z1_test[col + \"_sq\"] = Z1_test[col] ** 2\n",
    "\n",
    "run_log_reg(Z1_train, y_train, Z1_val, y_val, Z1_test, y_test, \"Z1: Squared Features credit_amount and age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z2: Cubic Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_train = X_train.copy()\n",
    "Z2_val = X_val.copy()\n",
    "Z2_test = X_test.copy()\n",
    "\n",
    "cubic_features = [\"duration\",\"credit_amount\",\"installment_commitment\",\"residence_since\", \"age\",\"existing_credits\",\"num_dependents\"] \n",
    "for col in cubic_features:\n",
    "    Z2_train[col + \"_cube\"] = Z2_train[col] ** 3\n",
    "    Z2_val[col + \"_cube\"] = Z2_val[col] ** 3\n",
    "    Z2_test[col + \"_cube\"] = Z2_test[col] ** 3\n",
    "\n",
    "run_log_reg(Z2_train, y_train, Z2_val, y_val, Z2_test, y_test, \"Z2: Cubic Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z3: Quartic Feature via Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z3_train = X_train.copy()\n",
    "Z3_val = X_val.copy()\n",
    "Z3_test = X_test.copy()\n",
    "\n",
    "f1 = \"duration\"\n",
    "f2 = \"credit_amount\"\n",
    "f3 = \"age\"\n",
    "f4 = \"installment_commitment\"\n",
    "\n",
    "Z3_train[f1 + \"_x_\" + f2 + \"_x_\" + f3 + \"_x_\" + f4] = Z3_train[f1] * Z3_train[f2] * Z3_train[f3] * Z3_train[f4]\n",
    "Z3_val[f1 + \"_x_\" + f2 + \"_x_\" + f3  + \"_x_\" + f4] = Z3_val[f1] * Z3_val[f2] * Z3_val[f3] * Z3_val[f4]\n",
    "Z3_test[f1 + \"_x_\" + f2 + \"_x_\" + f3  + \"_x_\" + f4] = Z3_test[f1] * Z3_test[f2] * Z3_test[f3] * Z3_test[f4]\n",
    "\n",
    "run_log_reg(Z3_train, y_train, Z3_val, y_val, Z3_test, y_test, \"Z3: Sum duration + credit_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_results_df = pd.DataFrame(log_reg_results).sort_values(['feature_version', 'lambda']).reset_index(drop=True)\n",
    "\n",
    "# Results table\n",
    "cols_to_show = [\n",
    "    'feature_version', 'lambda', \n",
    "    'val_acc', 'val_f1', 'val_roc_auc',\n",
    "    'test_acc', 'test_f1', 'test_roc_auc'\n",
    "]\n",
    "print(\"Logistic Regression results:\\n\")\n",
    "print(log_results_df[cols_to_show].to_string(index=False))\n",
    "\n",
    "log_results_df.to_csv('logreg_results.csv', index=False)\n",
    "print(\"Saved as 'logreg_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "metrics = [('val_acc', 'Validation Accuracy'), ('val_f1', 'Validation F1')]\n",
    "colors = {'Z0: Original Features': 'blue', 'Z1: Squared Features credit_amount and age': 'red', \n",
    "          'Z2: Cubic Features': 'green', 'Z3: Sum duration + credit_amount': 'orange'}\n",
    "\n",
    "for ax, (metric, title) in zip(axes, metrics):\n",
    "    for feat_name in log_results_df['feature_version'].unique():\n",
    "        data = log_results_df[log_results_df['feature_version'] == feat_name].sort_values('lambda')\n",
    "        ax.plot(data['lambda'], data[metric], marker='o', label=feat_name, color=colors.get(feat_name, None))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Lambda (C)', fontsize=12)\n",
    "    ax.set_ylabel(title, fontsize=12)\n",
    "    ax.set_title(f'{title} across feature sets', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logreg_metric_vs_lambda.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved plot as 'logreg_metric_vs_lambda.png'\")\n",
    "\n",
    "summary = log_results_df.groupby('feature_version')[['val_acc', 'val_f1', 'val_roc_auc']].agg(['mean', 'max']).round(4)\n",
    "print(\"Validation summary by feature set (mean and max):\")\n",
    "print(summary)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "metrics = [('val_acc', 'Validation Accuracy'), ('val_f1', 'Validation F1')]\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for ax, (metric, title) in zip(axes, metrics):\n",
    "    means = log_results_df.groupby('feature_version')[metric].mean()\n",
    "    ax.bar(means.index, means.values, color=colors)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(f'Average {title}')\n",
    "    ax.tick_params(axis='x', rotation=20, labelsize=8)\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logreg_transformation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved bar chart as 'logreg_transformation_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.2 Create Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_interact = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "X_train_base = np.asarray(X_train)\n",
    "X_val_base = np.asarray(X_val)\n",
    "X_test_base = np.asarray(X_test)\n",
    "\n",
    "\n",
    "def scale_after_transform(X_tr, X_v, X_te):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X_tr), scaler.transform(X_v), scaler.transform(X_te)\n",
    "\n",
    "X_train_poly_raw = poly.fit_transform(X_train_base)\n",
    "X_val_poly_raw = poly.transform(X_val_base)\n",
    "X_test_poly_raw = poly.transform(X_test_base)\n",
    "X_train_poly, X_val_poly, X_test_poly = scale_after_transform(\n",
    "    X_train_poly_raw, X_val_poly_raw, X_test_poly_raw\n",
    ")\n",
    "\n",
    "X_train_interact_raw = poly_interact.fit_transform(X_train_base)\n",
    "X_val_interact_raw = poly_interact.transform(X_val_base)\n",
    "X_test_interact_raw = poly_interact.transform(X_test_base)\n",
    "X_train_interact, X_val_interact, X_test_interact = scale_after_transform(\n",
    "    X_train_interact_raw, X_val_interact_raw, X_test_interact_raw\n",
    ")\n",
    "\n",
    "shift = X_train_base.min()\n",
    "X_train_sqrt_raw = np.sqrt(X_train_base - shift + 1)\n",
    "X_val_sqrt_raw = np.sqrt(X_val_base - shift + 1)\n",
    "X_test_sqrt_raw = np.sqrt(X_test_base - shift + 1)\n",
    "X_train_sqrt, X_val_sqrt, X_test_sqrt = scale_after_transform(\n",
    "    X_train_sqrt_raw, X_val_sqrt_raw, X_test_sqrt_raw\n",
    ")\n",
    "\n",
    "feature_sets = {\n",
    "    \"Original\": (X_train_base, X_val_base, X_test_base),\n",
    "    \"Polynomial (d=2)\": (X_train_poly, X_val_poly, X_test_poly),\n",
    "    \"Interactions\": (X_train_interact, X_val_interact, X_test_interact),\n",
    "    \"Square Root\": (X_train_sqrt, X_val_sqrt, X_test_sqrt),\n",
    "}\n",
    "\n",
    "for name, (Xt, Xv, Xte) in feature_sets.items():\n",
    "    print(f\"{name}: train {Xt.shape}, val {Xv.shape}, test {Xte.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.3 Define Regularization Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values_to_test = [3, 5, 7, 9, 11, 15]\n",
    "\n",
    "print(f\"K values: {k_values_to_test}\")\n",
    "print(\"Models to evaluate: 4 feature sets Ã— 6 K values = 24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.4 Train and Evaluate All Model Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "knn_results = []\n",
    "for feat_name, (X_tr, X_v, X_te) in feature_sets.items():\n",
    "    for k in k_values_to_test:\n",
    "        start = time.perf_counter()\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_tr, y_train)\n",
    "        training_time = time.perf_counter() - start\n",
    "\n",
    "        # Predictions\n",
    "        y_train_pred = knn.predict(X_tr)\n",
    "        y_val_pred = knn.predict(X_v)\n",
    "        y_test_pred = knn.predict(X_te)\n",
    "\n",
    "        y_train_proba = knn.predict_proba(X_tr)[:, 1]\n",
    "        y_val_proba = knn.predict_proba(X_v)[:, 1]\n",
    "        y_test_proba = knn.predict_proba(X_te)[:, 1]\n",
    "\n",
    "        knn_results.append({\n",
    "            'feature_version': feat_name,\n",
    "            'k': k,\n",
    "            'n_features': X_tr.shape[1],\n",
    "            'train_acc': accuracy_score(y_train, y_train_pred),\n",
    "            'val_acc': accuracy_score(y_val, y_val_pred),\n",
    "            'test_acc': accuracy_score(y_test, y_test_pred),\n",
    "            'val_precision': precision_score(y_val, y_val_pred),\n",
    "            'val_recall': recall_score(y_val, y_val_pred),\n",
    "            'val_f1': f1_score(y_val, y_val_pred),\n",
    "            'val_roc_auc': roc_auc_score(y_val, y_val_proba),\n",
    "            'test_precision': precision_score(y_test, y_test_pred),\n",
    "            'test_recall': recall_score(y_test, y_test_pred),\n",
    "            'test_f1': f1_score(y_test, y_test_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_test_proba),\n",
    "            'training_time': training_time,\n",
    "        })\n",
    "\n",
    "knn_results_df = pd.DataFrame(knn_results).sort_values(['feature_version', 'k']).reset_index(drop=True)\n",
    "results_df = knn_results_df.copy()\n",
    "\n",
    "best_model_config = knn_results_df.loc[knn_results_df['val_f1'].idxmax()]\n",
    "print(f\"Best validation F1: {best_model_config['val_f1']:.4f} using {best_model_config['feature_version']} with k={int(best_model_config['k'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.5 Complete Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_show = [\n",
    "    'feature_version', 'k', 'n_features',\n",
    "    'train_acc', 'val_acc', 'val_f1', 'val_roc_auc',\n",
    "    'test_f1', 'test_roc_auc', 'training_time'\n",
    "]\n",
    "print(\"KNN results (all 24 models):\\n\")\n",
    "print(knn_results_df[cols_to_show].to_string(index=False))\n",
    "\n",
    "knn_results_df.to_csv('knn_results.csv', index=False)\n",
    "print(\"Saved as 'knn_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.7 Visualize Performance vs Feature Transformations and K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "metrics = [('val_acc', 'Validation Accuracy'), ('val_f1', 'Validation F1')]\n",
    "colors = {'Original': 'blue', 'Polynomial (d=2)': 'red', 'Interactions': 'green', 'Square Root': 'orange'}\n",
    "\n",
    "for ax, (metric, title) in zip(axes, metrics):\n",
    "    for feat_name in feature_sets.keys():\n",
    "        data = knn_results_df[knn_results_df['feature_version'] == feat_name].sort_values('k')\n",
    "        ax.plot(data['k'], data[metric], marker='o', label=feat_name, color=colors.get(feat_name, None))\n",
    "    ax.set_xlabel('k (neighbors)', fontsize=12)\n",
    "    ax.set_ylabel(title, fontsize=12)\n",
    "    ax.set_title(f'{title} across feature sets', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('knn_metric_vs_k.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved plot as 'knn_metric_vs_k.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.8 Compare Performance Across Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = knn_results_df.groupby('feature_version')[['val_acc', 'val_f1', 'val_roc_auc']].agg(['mean', 'max']).round(4)\n",
    "print(\"Validation summary by feature set (mean and max):\")\n",
    "print(summary)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "metrics = [('val_acc', 'Validation Accuracy'), ('val_f1', 'Validation F1')]\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for ax, (metric, title) in zip(axes, metrics):\n",
    "    means = knn_results_df.groupby('feature_version')[metric].mean()\n",
    "    ax.bar(means.index, means.values, color=colors)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(f'Average {title}')\n",
    "    ax.tick_params(axis='x', rotation=20)\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('knn_transformation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved bar chart as 'knn_transformation_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN.10 Confusion Matrices for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for best model (validation and test)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for ax, (name, y_true, y_pred) in zip(\n",
    "    axes,\n",
    "    [('Validation', y_val, y_val_pred_best), ('Test', y_test, y_test_pred_best)]\n",
    "):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        xticklabels=['Bad (0)', 'Good (1)'],\n",
    "        yticklabels=['Bad (0)', 'Good (1)']\n",
    "    )\n",
    "    ax.set_title(f'{name} confusion matrix\\n{best_feat}, k={best_k}')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('knn_best_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved confusion matrices for validation and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.2 Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditRiskNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CreditRiskNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "test_model = CreditRiskNN(61)\n",
    "print(\"Model architecture defined successfully.\")\n",
    "print(f\"\\nModel structure:\")\n",
    "print(test_model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.3 Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(X_train, y_train, X_val, y_val, weight_decay=0.0, \n",
    "                   epochs=50, batch_size=32, patience=5):\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values if hasattr(y_train, 'values') else y_train).reshape(-1, 1).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = CreditRiskNN(X_train.shape[1]).to(device)\n",
    "    \n",
    "    # Loss and optimizer (weight_decay is L2 regularization)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            predictions = (outputs >= 0.5).float()\n",
    "            train_correct += (predictions == batch_y).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "            val_predictions = (val_outputs >= 0.5).float()\n",
    "            val_correct = (val_predictions == y_val_tensor).sum().item()\n",
    " \n",
    "        train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / len(y_val)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, train_time\n",
    "\n",
    "print(\"Training function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.4 Define Regularization Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay_values = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.5 Train and Evaluate All Model Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_all_results = []\n",
    "\n",
    "for feat_name, (X_tr, X_v, X_te) in feature_sets.items():\n",
    "    for wd in weight_decay_values:\n",
    "        model, history, train_time = train_nn_model(\n",
    "            X_tr, y_train, X_v, y_val,\n",
    "            weight_decay=wd,\n",
    "            epochs=200,\n",
    "            batch_size=32,\n",
    "            patience=5\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_train_pred_proba = model(torch.FloatTensor(X_tr).to(device)).cpu().numpy().flatten()\n",
    "            y_val_pred_proba = model(torch.FloatTensor(X_v).to(device)).cpu().numpy().flatten()\n",
    "            y_test_pred_proba = model(torch.FloatTensor(X_te).to(device)).cpu().numpy().flatten()\n",
    "\n",
    "        y_train_pred = (y_train_pred_proba > 0.5).astype(int)\n",
    "        y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n",
    "        y_test_pred = (y_test_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        val_f1 = f1_score(y_val, y_val_pred)\n",
    "        n_epochs = len(history['train_loss'])\n",
    "\n",
    "        nn_all_results.append({\n",
    "            'feature_version': feat_name,\n",
    "            'weight_decay': wd,\n",
    "            'n_features': X_tr.shape[1],\n",
    "            'n_epochs': n_epochs,\n",
    "            'train_acc': accuracy_score(y_train, y_train_pred),\n",
    "            'val_acc': accuracy_score(y_val, y_val_pred),\n",
    "            'test_acc': accuracy_score(y_test, y_test_pred),\n",
    "            'val_precision': precision_score(y_val, y_val_pred),\n",
    "            'val_recall': recall_score(y_val, y_val_pred),\n",
    "            'val_f1': val_f1,\n",
    "            'val_roc_auc': roc_auc_score(y_val, y_val_pred_proba),\n",
    "            'test_precision': precision_score(y_test, y_test_pred),\n",
    "            'test_recall': recall_score(y_test, y_test_pred),\n",
    "            'test_f1': f1_score(y_test, y_test_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_test_pred_proba),\n",
    "            'training_time': train_time,\n",
    "            'final_train_loss': history['train_loss'][-1],\n",
    "            'final_val_loss': history['val_loss'][-1]\n",
    "        })\n",
    "\n",
    "nn_all_results_df = pd.DataFrame(nn_all_results)\n",
    "print(f\"Completed {len(nn_all_results_df)} neural network models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.6 Complete Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_show = [\n",
    "    'feature_version', 'weight_decay', 'n_features', 'n_epochs',\n",
    "    'train_acc', 'val_acc', 'val_f1', 'test_acc', 'test_f1', 'test_roc_auc'\n",
    "]\n",
    "print(\"Neural network results (all combinations):\\n\")\n",
    "print(nn_all_results_df[cols_to_show].to_string(index=False))\n",
    "\n",
    "nn_all_results_df.to_csv('nn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.8 Visualize Performance vs Feature Transformations and Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = [\n",
    "    ('val_acc', 'Validation Accuracy', axes[0, 0]),\n",
    "    ('val_f1', 'Validation F1-Score', axes[0, 1]),\n",
    "    ('val_roc_auc', 'Validation ROC-AUC', axes[1, 0]),\n",
    "    ('training_time', 'Training Time (seconds)', axes[1, 1])\n",
    "]\n",
    "\n",
    "colors = {'Original': 'blue', 'Polynomial (d=2)': 'red', \n",
    "          'Interactions': 'green', 'Square Root': 'orange'}\n",
    "markers = {'Original': 'o', 'Polynomial (d=2)': 's', \n",
    "           'Interactions': '^', 'Square Root': 'D'}\n",
    "\n",
    "# X-axis labels for weight decay values\n",
    "wd_labels = [str(wd) for wd in weight_decay_values]\n",
    "\n",
    "for metric, title, ax in metrics:\n",
    "    for feat_name in nn_all_results_df['feature_version'].unique():\n",
    "        data = nn_all_results_df[nn_all_results_df['feature_version'] == feat_name].sort_values('weight_decay')\n",
    "        \n",
    "        ax.plot(range(len(weight_decay_values)), data[metric], \n",
    "                marker=markers[feat_name], \n",
    "                color=colors[feat_name],\n",
    "                linewidth=2, markersize=8, label=feat_name)\n",
    "    \n",
    "    ax.set_xlabel('Weight Decay (L2 Regularization)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{title} vs Weight Decay\\nand Feature Transformations', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(range(len(weight_decay_values)))\n",
    "    ax.set_xticklabels(wd_labels, rotation=45, ha='right')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for best model\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_best['train_loss']) + 1)\n",
    "\n",
    "ax.plot(epochs_range, history_best['train_loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "ax.plot(epochs_range, history_best['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Loss (Binary Cross Entropy)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
